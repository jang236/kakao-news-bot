import os
import re
import subprocess
import json as json_module
import google.generativeai as genai
from fastapi import FastAPI
from pydantic import BaseModel
import requests
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi

# ===== Gemini API ì„¤ì • =====
API_KEY = os.environ.get("GEMINI_API_KEY", "")
genai.configure(api_key=API_KEY)
model = genai.GenerativeModel("gemini-3-flash-preview")

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ë¥¼ ê²½ì œì  ê´€ì ì—ì„œ í•´ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ê·œì¹™]
- ëª¨ë“  ë¶„ì•¼ì˜ ë‰´ìŠ¤(ì •ì¹˜, ì‚¬íšŒ, ê¸°ìˆ , êµ­ì œ ë“±)ë¥¼ ë°›ë˜, ê²½ì œ/íˆ¬ìì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„
- ì…ë ¥ëœ ë‰´ìŠ¤ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ 3íšŒ ì••ì¶•: íˆ¬ì ê´€ë ¨ í•„í„°ë§ â†’ ì¸ê³¼ê´€ê³„ êµ¬ì¡°í™” â†’ í•µì‹¬ íŒë‹¨ ìš”ì†Œ ì¶”ì¶œ. ì´ ê³¼ì •ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
- ìš”ì•½ì—ì„œëŠ” ì „ë¬¸ ìš©ì–´ë¥¼ ê´„í˜¸ ì„¤ëª… ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©. ìš©ì–´ ì„¤ëª…ì€ ğŸ“– ìš©ì–´ ì„¹ì…˜ì—ì„œ ë”°ë¡œ ì œê³µ
- ì¹œê·¼í•œ ë§íˆ¬ (~ê±°ë“ ìš”, ~ë€ ë§ì´ì—ìš”, ~ê±°ì˜ˆìš”)
- í™•ì •ì  í‘œí˜„ ê¸ˆì§€, ê°€ëŠ¥ì„±ìœ¼ë¡œ í‘œí˜„
- ë§ˆí¬ë‹¤ìš´(**, *, -, ë²ˆí˜¸ ëª©ë¡ ë“±) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€. ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
- ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ì´ë¯€ë¡œ ìµœëŒ€í•œ ì§§ê²Œ ì‘ì„±
- ê° ì„¹ì…˜ ì‚¬ì´ì— ë¹ˆ ì¤„ì„ ë„£ì–´ ë¬¸ë‹¨ì„ ëª…í™•íˆ êµ¬ë¶„

[ì¶œë ¥ í˜•ì‹ â€” ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ]

ğŸ“° (ì œê³µëœ ì œëª©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©. ì ˆëŒ€ ìˆ˜ì •í•˜ê±°ë‚˜ ìš”ì•½í•˜ì§€ ë§ ê²ƒ)

âœ… ìš”ì•½ (ğŸŸ¢ê¸ì • / ğŸ”´ë¶€ì • / ğŸŸ¡ì¤‘ë¦½):
í•µì‹¬ ë‚´ìš© 3~4ë¬¸ì¥. ê° ë¬¸ì¥ ì‚¬ì´ì— ë¹ˆ ì¤„(ì—”í„° 2ë²ˆ)ì„ ë„£ì–´ ë¬¸ë‹¨ì„ êµ¬ë¶„í•  ê²ƒ.
ê´„í˜¸ ì„¤ëª… ë„£ì§€ ë§ ê²ƒ. ëˆ„ê°€ ë¬´ì—‡ì„ ì™œ í–ˆëŠ”ì§€, ê²½ì œ/ì‹œì¥ì— ì–´ë–¤ ì˜í–¥ì¸ì§€.

ğŸ“– ìš©ì–´: ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ = ì‰¬ìš´ ì„¤ëª… (ìµœëŒ€ 2~3ê°œë§Œ, ì‰¬ìš´ ë‰´ìŠ¤ëŠ” ìƒëµ ê°€ëŠ¥)

ğŸ¤– AI í•œì¤„í‰: ë‰´ìŠ¤ì˜ í•µì‹¬ì„ ë¹„ìœ ë‚˜ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ í•œ ì¤„ ì •ë¦¬. ëˆ„êµ¬ë‚˜ "ì•„, ê·¸ëŸ° ê±°êµ¬ë‚˜" í•˜ê³  ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆê²Œ.

ğŸ·ï¸ ê´€ë ¨ ì„¹í„°: (ì˜í–¥ ë°›ëŠ” ì‚°ì—…/ì„¹í„° ë‚˜ì—´)

[ì˜ˆì™¸ ì²˜ë¦¬]
- ë‰´ìŠ¤ê°€ ì•„ë‹Œ ë‚´ìš©ì´ ì…ë ¥ëœ ê²½ìš°: "ë‰´ìŠ¤ ë˜ëŠ” ìœ íŠœë¸Œ URLì„ ë„£ì–´ì£¼ì‹œë©´ ë¶„ì„í•´ë“œë¦´ê²Œìš”!"
- ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš°: "ë‚´ìš©ì´ ë¶€ì¡±í•´ìš”. ë‹¤ë¥¸ URLì„ ë„£ì–´ì£¼ì‹œë©´ ë¶„ì„í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”!"

ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

app = FastAPI()


class Message(BaseModel):
    text: str


def extract_article(url: str) -> dict:
    """URLì—ì„œ ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/120.0.0.0 Safari/537.36"
    }
    resp = requests.get(url, headers=headers, timeout=10)
    resp.encoding = resp.apparent_encoding
    soup = BeautifulSoup(resp.text, "html.parser")

    # ê¸°ì‚¬ ì œëª© ì¶”ì¶œ
    title = ""
    title_el = soup.select_one("#title_area, .media_end_head_headline, "
                                "#articleTitle, h1.headline, .article_tit, "
                                "h1#articleTitle, .tit_view")
    if title_el:
        title = title_el.get_text(strip=True)
    if not title:
        og_title = soup.find("meta", property="og:title")
        if og_title and og_title.get("content"):
            title = og_title["content"]
    if not title and soup.title:
        title = soup.title.get_text(strip=True)

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
    for tag in soup(["script", "style", "nav", "header", "footer", "aside", "iframe"]):
        tag.decompose()

    # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
    article = soup.select_one("#dic_area, #articleBodyContents, .article_body, "
                               "article, .news_end, #articeBody, #newsEndContents")
    if article:
        text = article.get_text(strip=True, separator="\n")
    else:
        text = soup.get_text(strip=True, separator="\n")

    # ë¹ˆ ì¤„ ì •ë¦¬ ë° ê¸¸ì´ ì œí•œ
    lines = [line.strip() for line in text.split("\n") if line.strip()]
    body = "\n".join(lines)[:4000]

    return {"title": title, "body": body}


def is_url(text: str) -> bool:
    """í…ìŠ¤íŠ¸ê°€ URLì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return bool(re.match(r"https?://", text.strip()))


def is_youtube_url(url: str) -> bool:
    """ìœ íŠœë¸Œ URLì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return bool(re.match(r"https?://(www\.)?(youtube\.com|youtu\.be|m\.youtube\.com)/", url.strip()))


def extract_video_id(url: str) -> str:
    """ìœ íŠœë¸Œ URLì—ì„œ ë¹„ë””ì˜¤ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    patterns = [
        r"(?:v=|/v/|youtu\.be/|/embed/|/shorts/)([a-zA-Z0-9_-]{11})",
        r"([a-zA-Z0-9_-]{11})"
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return ""


def extract_youtube(url: str) -> dict:
    """ìœ íŠœë¸Œ ì˜ìƒì—ì„œ ì œëª©ê³¼ ìë§‰ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    video_id = extract_video_id(url)
    if not video_id:
        return {"title": "", "body": ""}

    # oEmbed APIë¡œ ì œëª© ì¶”ì¶œ
    title = ""
    try:
        oembed_url = f"https://www.youtube.com/oembed?url=https://www.youtube.com/watch?v={video_id}&format=json"
        resp = requests.get(oembed_url, timeout=5)
        if resp.status_code == 200:
            title = resp.json().get("title", "")
    except Exception:
        pass

    # ìë§‰ ì¶”ì¶œ â€” 1ì°¨: youtube-transcript-api (v1.x API)
    transcript_text = ""
    try:
        ytt_api = YouTubeTranscriptApi()
        # í•œêµ­ì–´ ìš°ì„ , ì˜ì–´ í´ë°±
        fetched = ytt_api.fetch(video_id, languages=['ko', 'en'])
        transcript_text = " ".join([snippet.text for snippet in fetched])[:4000]
    except Exception:
        pass

    # 1ì°¨ ì‹¤íŒ¨ ì‹œ â€” listë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ íƒìƒ‰
    if not transcript_text:
        try:
            ytt_api = YouTubeTranscriptApi()
            transcript_list = ytt_api.list(video_id)
            # ì•„ë¬´ ìë§‰ì´ë‚˜ ê°€ì ¸ì˜¤ê¸°
            for transcript in transcript_list:
                fetched = transcript.fetch()
                transcript_text = " ".join([snippet.text for snippet in fetched])[:4000]
                break
        except Exception:
            pass

    # ìë§‰ ì¶”ì¶œ â€” 2ì°¨: yt-dlp í´ë°±
    if not transcript_text:
        try:
            cmd = [
                "yt-dlp",
                "--write-auto-sub",
                "--sub-lang", "ko,en",
                "--skip-download",
                "--sub-format", "json3",
                "-o", "/tmp/%(id)s",
                f"https://www.youtube.com/watch?v={video_id}"
            ]
            subprocess.run(cmd, capture_output=True, timeout=30)

            # ìë§‰ íŒŒì¼ ì°¾ê¸°
            import glob
            sub_files = glob.glob(f"/tmp/{video_id}*.json3")
            if sub_files:
                with open(sub_files[0], "r", encoding="utf-8") as f:
                    sub_data = json_module.load(f)
                events = sub_data.get("events", [])
                texts = []
                for event in events:
                    segs = event.get("segs", [])
                    for seg in segs:
                        t = seg.get("utf8", "").strip()
                        if t and t != "\n":
                            texts.append(t)
                transcript_text = " ".join(texts)[:4000]

                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for f in sub_files:
                    os.remove(f)
        except Exception:
            pass

    return {"title": title, "body": transcript_text}


def analyze_content(url: str) -> str:
    """URLì„ ë¶„ì„í•©ë‹ˆë‹¤ (ë‰´ìŠ¤ ë˜ëŠ” ìœ íŠœë¸Œ ìë™ íŒë³„)."""
    try:
        # YouTube vs ë‰´ìŠ¤ ë¶„ê¸°
        if is_youtube_url(url):
            content = extract_youtube(url)
            content_type = "ìœ íŠœë¸Œ ì˜ìƒ"
            if not content["body"]:
                return "âš ï¸ ìë§‰ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ì˜ìƒì´ì—ìš”. ìë§‰ì´ ìˆëŠ” ì˜ìƒì„ ë„£ì–´ì£¼ì„¸ìš”!"
        else:
            content = extract_article(url)
            content_type = "ë‰´ìŠ¤ ê¸°ì‚¬"

        if len(content["body"]) < 50:
            return f"âš ï¸ {content_type} ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ì£¼ì„¸ìš”."

        prompt = f"{SYSTEM_PROMPT}\n\n---\nì œëª©: {content['title']}\n\në‚´ìš©:\n{content['body']}"
        response = model.generate_content(prompt)
        return response.text

    except requests.exceptions.Timeout:
        return "âš ï¸ ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except requests.exceptions.RequestException as e:
        return f"âš ï¸ URL ì ‘ì† ì˜¤ë¥˜: {str(e)}"
    except Exception as e:
        return f"âš ï¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"


@app.get("/")
async def root():
    return {"status": "ok", "message": "ë‰´ìŠ¤ ë¶„ì„ ë´‡ ì„œë²„ ì‘ë™ ì¤‘"}


@app.post("/analyze")
async def analyze(msg: Message):
    text = msg.text.strip()

    # "ë¶„ì„" í‚¤ì›Œë“œ ì œê±°
    if text.startswith("ë¶„ì„ "):
        text = text.replace("ë¶„ì„ ", "", 1).strip()

    if not is_url(text):
        return {"response": "âš ï¸ ì˜¬ë°”ë¥¸ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\në‰´ìŠ¤ ë˜ëŠ” ìœ íŠœë¸Œ URLì„ ë„£ì–´ì£¼ì„¸ìš”!"}

    result = analyze_content(text)
    return {"response": result}


@app.get("/health")
async def health():
    return {"status": "ok"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
